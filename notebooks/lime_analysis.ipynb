{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7555a27c",
   "metadata": {},
   "source": [
    "# LIME Analysis for News Bias Classification\n",
    "\n",
    "This notebook explores using LIME (Local Interpretable Model-agnostic Explanations) to understand why our bias classification model makes certain predictions.\n",
    "\n",
    "We'll focus on creating **user-friendly explanations** suitable for web applications rather than technical research graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install lime\n",
    "!pip install transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the backend directory to path to import functions\n",
    "sys.path.append('../backend')\n",
    "\n",
    "# Load the bias classification model\n",
    "BIAS_MODEL_DIR = \"../backend/bias_classification_model\"\n",
    "\n",
    "try:\n",
    "    bias_model = TFDistilBertForSequenceClassification.from_pretrained(BIAS_MODEL_DIR)\n",
    "    bias_tokenizer = DistilBertTokenizer.from_pretrained(BIAS_MODEL_DIR)\n",
    "    print(\"✅ News Bias model and tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label meanings (update based on your actual model)\n",
    "BIAS_LABEL_MEANINGS = {\n",
    "    0: \"Republican\",\n",
    "    1: \"Liberal\", \n",
    "    2: \"Neutral\",\n",
    "    3: \"Other\"\n",
    "}\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text for model input\"\"\"\n",
    "    text = re.sub(r'[\\'\"\"]+', '\"', text)\n",
    "    text = re.sub(r'[\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bias_probabilities(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict bias probabilities for a list of texts.\n",
    "    This function is required by LIME.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to classify\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of shape (n_samples, n_classes) with probabilities\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for text in texts:\n",
    "        cleaned_text = clean_text(text)\n",
    "        inputs = bias_tokenizer(\n",
    "            cleaned_text, \n",
    "            return_tensors=\"tf\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        outputs = bias_model(inputs)\n",
    "        probabilities = tf.nn.softmax(outputs.logits).numpy()[0]\n",
    "        predictions.append(probabilities)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prediction function\n",
    "test_texts = [\n",
    "    \"The healthcare policy will provide universal coverage.\",\n",
    "    \"The radical left-wing agenda is destroying our nation!\"\n",
    "]\n",
    "\n",
    "predictions = predict_bias_probabilities(test_texts)\n",
    "print(\"Prediction shape:\", predictions.shape)\n",
    "print(\"\\nPredictions:\")\n",
    "for i, (text, pred) in enumerate(zip(test_texts, predictions)):\n",
    "    predicted_label = np.argmax(pred)\n",
    "    confidence = pred[predicted_label] * 100\n",
    "    print(f\"Text {i+1}: {BIAS_LABEL_MEANINGS[predicted_label]} ({confidence:.1f}%)\")\n",
    "    print(f\"  Text: {text[:50]}...\")\n",
    "    print(f\"  All probabilities: {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(\n",
    "    class_names=list(BIAS_LABEL_MEANINGS.values()),\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "print(\"✅ LIME explainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_with_lime(text: str, num_features: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a text using LIME and return user-friendly explanations.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        num_features: Number of most important features to show\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with explanation data suitable for web display\n",
    "    \"\"\"\n",
    "    # Get LIME explanation\n",
    "    explanation = explainer.explain_instance(\n",
    "        text, \n",
    "        predict_bias_probabilities, \n",
    "        num_features=num_features,\n",
    "        num_samples=500  # Reduce for faster processing\n",
    "    )\n",
    "    \n",
    "    # Get model prediction\n",
    "    prediction = predict_bias_probabilities([text])[0]\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[predicted_class] * 100\n",
    "    \n",
    "    # Extract feature importance for the predicted class\n",
    "    feature_importance = explanation.as_list()\n",
    "    \n",
    "    # Separate positive and negative influences\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    \n",
    "    for word, importance in feature_importance:\n",
    "        if importance > 0:\n",
    "            positive_words.append((word, importance))\n",
    "        else:\n",
    "            negative_words.append((word, abs(importance)))\n",
    "    \n",
    "    # Sort by importance\n",
    "    positive_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    negative_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create highlighted text\n",
    "    highlighted_text = create_highlighted_text(text, feature_importance)\n",
    "    \n",
    "    # Generate simple explanation\n",
    "    simple_explanation = generate_simple_explanation(\n",
    "        predicted_class, positive_words, negative_words\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'predicted_class': predicted_class,\n",
    "        'predicted_label': BIAS_LABEL_MEANINGS[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'highlighted_text': highlighted_text,\n",
    "        'simple_explanation': simple_explanation,\n",
    "        'positive_influences': positive_words[:5],  # Top 5\n",
    "        'negative_influences': negative_words[:5],  # Top 5\n",
    "        'all_probabilities': {BIAS_LABEL_MEANINGS[i]: float(prob * 100) for i, prob in enumerate(prediction)}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c973db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_highlighted_text(text: str, feature_importance: List[Tuple[str, float]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create highlighted text data for frontend display.\n",
    "    \n",
    "    Returns list of dictionaries with word and importance level.\n",
    "    \"\"\"\n",
    "    # Create importance lookup\n",
    "    importance_dict = {word: importance for word, importance in feature_importance}\n",
    "    \n",
    "    # Split text into words (simple approach)\n",
    "    words = text.split()\n",
    "    highlighted = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Clean word for lookup (remove punctuation)\n",
    "        clean_word = re.sub(r'[^\\w]', '', word.lower())\n",
    "        importance = importance_dict.get(clean_word, 0)\n",
    "        \n",
    "        # Determine highlight level\n",
    "        if importance > 0.1:\n",
    "            highlight_level = 'high_positive'\n",
    "        elif importance > 0.05:\n",
    "            highlight_level = 'medium_positive'\n",
    "        elif importance < -0.1:\n",
    "            highlight_level = 'high_negative'\n",
    "        elif importance < -0.05:\n",
    "            highlight_level = 'medium_negative'\n",
    "        else:\n",
    "            highlight_level = 'neutral'\n",
    "        \n",
    "        highlighted.append({\n",
    "            'word': word,\n",
    "            'importance': importance,\n",
    "            'highlight_level': highlight_level\n",
    "        })\n",
    "    \n",
    "    return highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_explanation(predicted_class: int, positive_words: List, negative_words: List) -> str:\n",
    "    \"\"\"\n",
    "    Generate a simple, user-friendly explanation of the model's decision.\n",
    "    \"\"\"\n",
    "    label = BIAS_LABEL_MEANINGS[predicted_class]\n",
    "    \n",
    "    explanation = f\"The model classified this text as '{label}' because it \"\n",
    "    \n",
    "    if positive_words:\n",
    "        top_positive = [word for word, _ in positive_words[:3]]\n",
    "        explanation += f\"focused on words like '{\"', '\".join(top_positive)}' which \"\n",
    "        explanation += f\"strongly indicate {label.lower()} bias. \"\n",
    "    \n",
    "    if negative_words:\n",
    "        top_negative = [word for word, _ in negative_words[:3]]\n",
    "        explanation += f\"However, words like '{\"', '\".join(top_negative)}' \"\n",
    "        explanation += f\"work against this classification. \"\n",
    "    \n",
    "    return explanation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the problematic text\n",
    "problematic_text = \"The radical left-wing agenda is destroying our great nation! These socialist policies will bankrupt America and take away our freedoms. We must stop this madness before it's too late!\"\n",
    "\n",
    "print(\"Analyzing problematic text with LIME...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "lime_result = analyze_text_with_lime(problematic_text)\n",
    "\n",
    "print(\"=== LIME Analysis Results ===\")\n",
    "print(f\"Predicted Label: {lime_result['predicted_label']}\")\n",
    "print(f\"Confidence: {lime_result['confidence']:.1f}%\")\n",
    "print(f\"\\nSimple Explanation: {lime_result['simple_explanation']}\")\n",
    "\n",
    "print(\"\\n=== Words Supporting This Classification ===\")\n",
    "for word, importance in lime_result['positive_influences']:\n",
    "    print(f\"  '{word}': {importance:.3f}\")\n",
    "\n",
    "print(\"\\n=== Words Working Against This Classification ===\")\n",
    "for word, importance in lime_result['negative_influences']:\n",
    "    print(f\"  '{word}': {importance:.3f}\")\n",
    "\n",
    "print(\"\\n=== All Class Probabilities ===\")\n",
    "for label, prob in lime_result['all_probabilities'].items():\n",
    "    print(f\"  {label}: {prob:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display highlighted text (simulated for web)\n",
    "print(\"\\n=== Text Highlighting (for Web Display) ===\")\n",
    "print(\"Green = Supports classification, Red = Works against classification\\n\")\n",
    "\n",
    "for word_data in lime_result['highlighted_text']:\n",
    "    word = word_data['word']\n",
    "    level = word_data['highlight_level']\n",
    "    importance = word_data['importance']\n",
    "    \n",
    "    if level.endswith('positive'):\n",
    "        color = '🟢' if 'high' in level else '🟡'\n",
    "    elif level.endswith('negative'):\n",
    "        color = '🔴' if 'high' in level else '🟠'\n",
    "    else:\n",
    "        color = '⚪'\n",
    "    \n",
    "    print(f\"{color} {word}\", end=\" \")\n",
    "\n",
    "print(\"\\n\\nLegend:\")\n",
    "print(\"🟢 Strongly supports classification\")\n",
    "print(\"🟡 Moderately supports classification\")\n",
    "print(\"🔴 Strongly works against classification\")\n",
    "print(\"🟠 Moderately works against classification\")\n",
    "print(\"⚪ Neutral/minimal impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with neutral text\n",
    "neutral_text = \"The new healthcare policy introduced by the government aims to provide universal coverage for all citizens. This comprehensive reform will ensure that no one is left behind and healthcare becomes a fundamental right for everyone.\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYZING NEUTRAL TEXT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "neutral_result = analyze_text_with_lime(neutral_text, num_features=8)\n",
    "\n",
    "print(f\"Predicted Label: {neutral_result['predicted_label']}\")\n",
    "print(f\"Confidence: {neutral_result['confidence']:.1f}%\")\n",
    "print(f\"\\nSimple Explanation: {neutral_result['simple_explanation']}\")\n",
    "\n",
    "print(\"\\n=== Most Influential Words ===\")\n",
    "for word, importance in neutral_result['positive_influences'][:3]:\n",
    "    print(f\"  '{word}': {importance:.3f} (supports classification)\")\n",
    "for word, importance in neutral_result['negative_influences'][:3]:\n",
    "    print(f\"  '{word}': {importance:.3f} (works against)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d6fbb",
   "metadata": {},
   "source": [
    "## Web Implementation Strategy\n",
    "\n",
    "Based on this analysis, here's how we should implement LIME in the web application:\n",
    "\n",
    "### 1. **User-Friendly Approach**\n",
    "- **Text Highlighting**: Color-code words based on their influence\n",
    "- **Simple Explanations**: Plain English explanations of the model's decision\n",
    "- **Top Influential Words**: Show 3-5 most important words with explanations\n",
    "\n",
    "### 2. **API Response Structure**\n",
    "```json\n",
    "{\n",
    "  \"classification\": {\n",
    "    \"label\": \"Liberal\",\n",
    "    \"confidence\": 65.2\n",
    "  },\n",
    "  \"explanation\": {\n",
    "    \"simple_text\": \"The model focused on words like 'radical', 'socialist'...\",\n",
    "    \"highlighted_words\": [\n",
    "      {\"word\": \"radical\", \"importance\": 0.15, \"highlight_level\": \"high_positive\"},\n",
    "      {\"word\": \"left-wing\", \"importance\": 0.12, \"highlight_level\": \"high_positive\"}\n",
    "    ],\n",
    "    \"key_influences\": {\n",
    "      \"supporting\": [(\"radical\", 0.15), (\"socialist\", 0.12)],\n",
    "      \"opposing\": [(\"great\", 0.08), (\"nation\", 0.06)]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **Frontend Display**\n",
    "- **Highlighted Text**: Show original text with color-coded words\n",
    "- **Explanation Box**: Simple explanation in conversational language\n",
    "- **Advanced Toggle**: Optional detailed view for technical users\n",
    "\n",
    "This approach makes AI explanations accessible to general users while still providing the insights needed to debug model behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
