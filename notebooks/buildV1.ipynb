{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\semester_6\\nlp\\news_bias\\virtual\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports and Configuration ---\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "INPUT_FILE = 'preprocessed_data.parquet'\n",
    "OUTPUT_DIR = 'bias_classification_model' # Directory to save the model and tokenizer\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "EXPECTED_LABELS = {0, 1, 2, 3} # Define expected labels clearly\n",
    "NUM_LABELS = len(EXPECTED_LABELS) # Determine num_labels from expected labels\n",
    "\n",
    "print(\"Libraries imported and configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not detected. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: GPU Setup ---\n",
    "print(\"Available GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Configure memory growth for each GPU\n",
    "        for gpu in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configured memory growth for {len(physical_devices)} GPU(s).\")\n",
    "        # Optional: Set specific GPU device if needed (usually TF handles this)\n",
    "        # tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"RuntimeError configuring GPU: {e}\")\n",
    "        print(\"Ensure this cell is run before any TensorFlow operations that initialize the GPU.\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f16d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 9)\n",
      "┌────────────┬──────────┬────────────┬───────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ content    ┆ outlet   ┆ original_i ┆ label ┆ … ┆ tokens     ┆ tokens_no_ ┆ tokens_le ┆ tokens_fi │\n",
      "│ ---        ┆ ---      ┆ ndex       ┆ ---   ┆   ┆ ---        ┆ stop       ┆ mmatized  ┆ ltered    │\n",
      "│ str        ┆ str      ┆ ---        ┆ i64   ┆   ┆ list[str]  ┆ ---        ┆ ---       ┆ ---       │\n",
      "│            ┆          ┆ i64        ┆       ┆   ┆            ┆ list[str]  ┆ list[str] ┆ list[str] │\n",
      "╞════════════╪══════════╪════════════╪═══════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ FBI        ┆ BBC News ┆ 800        ┆ 2     ┆ … ┆ [\"fbi\",    ┆ [\"fbi\",    ┆ [\"fbi\",   ┆ [\"fbi\",   │\n",
      "│ arrests    ┆          ┆            ┆       ┆   ┆ \"arrests\", ┆ \"arrests\", ┆ \"arrest\", ┆ \"arrest\", │\n",
      "│ so-called  ┆          ┆            ┆       ┆   ┆ …          ┆ …          ┆ … \"capito ┆ … \"capito │\n",
      "│ sedition…  ┆          ┆            ┆       ┆   ┆ \"capitol\"… ┆ \"capitol\"… ┆ l\"]       ┆ l\"]       │\n",
      "│ House      ┆ BBC News ┆ 801        ┆ 0     ┆ … ┆ [\"house\",  ┆ [\"house\",  ┆ [\"house\", ┆ [\"house\", │\n",
      "│ Speaker    ┆          ┆            ┆       ┆   ┆ \"speaker\", ┆ \"speaker\", ┆ \"speaker\" ┆ \"speaker\" │\n",
      "│ defends    ┆          ┆            ┆       ┆   ┆ … \"commen… ┆ … \"commen… ┆ , …       ┆ , …       │\n",
      "│ giving C…  ┆          ┆            ┆       ┆   ┆            ┆            ┆ \"commen…  ┆ \"commen…  │\n",
      "│ Capitol    ┆ BBC News ┆ 802        ┆ 0     ┆ … ┆ [\"capitol\" ┆ [\"capitol\" ┆ [\"capitol ┆ [\"capitol │\n",
      "│ rioter who ┆          ┆            ┆       ┆   ┆ ,          ┆ ,          ┆ \",        ┆ \",        │\n",
      "│ jabbed     ┆          ┆            ┆       ┆   ┆ \"rioter\",  ┆ \"rioter\",  ┆ \"rioter\", ┆ \"rioter\", │\n",
      "│ Conf…      ┆          ┆            ┆       ┆   ┆ … \"far\"]   ┆ … \"far\"]   ┆ … \"far\"]  ┆ … \"far\"]  │\n",
      "│ Capitol    ┆ BBC News ┆ 803        ┆ 0     ┆ … ┆ [\"capitol\" ┆ [\"capitol\" ┆ [\"capitol ┆ [\"capitol │\n",
      "│ rioter who ┆          ┆            ┆       ┆   ┆ ,          ┆ ,          ┆ \",        ┆ \",        │\n",
      "│ posed with ┆          ┆            ┆       ┆   ┆ \"rioter\",  ┆ \"rioter\",  ┆ \"rioter\", ┆ \"rioter\", │\n",
      "│ …          ┆          ┆            ┆       ┆   ┆ … \"priso…  ┆ … \"priso…  ┆ … \"priso… ┆ … \"priso… │\n",
      "│ Proud Boy  ┆ BBC News ┆ 804        ┆ 1     ┆ … ┆ [\"proud\",  ┆ [\"proud\",  ┆ [\"proud\", ┆ [\"proud\", │\n",
      "│ leader     ┆          ┆            ┆       ┆   ┆ \"boy\", …   ┆ \"boy\", …   ┆ \"boy\", …  ┆ \"boy\", …  │\n",
      "│ spoke of   ┆          ┆            ┆       ┆   ┆ \"years\"]   ┆ \"years\"]   ┆ \"year\"]   ┆ \"year\"]   │\n",
      "│ 'war…      ┆          ┆            ┆       ┆   ┆            ┆            ┆           ┆           │\n",
      "└────────────┴──────────┴────────────┴───────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Load Data ---\n",
    "import polars as pl # Make sure polars is imported\n",
    "\n",
    "# Assume INPUT_FILE is defined in Cell 1\n",
    "# INPUT_FILE = 'preprocessed_data.parquet'\n",
    "\n",
    "print(f\"Loading data from {INPUT_FILE}...\")\n",
    "try:\n",
    "    df = pl.read_parquet(INPUT_FILE)\n",
    "    print(f\"Data loaded successfully. Initial shape: {df.shape}\")\n",
    "\n",
    "    # Display schema and first few rows\n",
    "    print(\"\\nData Schema:\")\n",
    "    # Correct way to display schema in Polars: access the .schema attribute\n",
    "    print(df.schema)\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head()) # .head() works correctly\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {INPUT_FILE}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing data: {e}\")\n",
    "    raise # Reraise the exception to stop execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187966f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for easier integration with TensorFlow\n",
    "pandas_df = df.to_pandas()\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='tf')\n",
    "\n",
    "# Tokenize the clean_content column\n",
    "tokenized_data = [tokenize_function(text) for text in pandas_df['clean_content']]\n",
    "\n",
    "# Extract input_ids and attention_masks\n",
    "input_ids = tf.stack([data['input_ids'][0] for data in tokenized_data])\n",
    "attention_masks = tf.stack([data['attention_mask'][0] for data in tokenized_data])\n",
    "labels = tf.convert_to_tensor(pandas_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Data Cleaning and Validation ---\n",
    "\n",
    "print(\"Starting data cleaning and validation...\")\n",
    "# Check for missing values\n",
    "missing_content = df.filter(pl.col(\"clean_content\").is_null() | (pl.col(\"clean_content\") == \"\")).height\n",
    "missing_label = df.filter(pl.col(\"label\").is_null()).height\n",
    "print(f\"Rows with missing/empty 'clean_content': {missing_content}\")\n",
    "print(f\"Rows with missing 'label': {missing_label}\")\n",
    "\n",
    "# Remove rows with missing values\n",
    "initial_rows = df.height\n",
    "df = df.filter(pl.col(\"clean_content\").is_not_null() & (pl.col(\"clean_content\") != \"\") & pl.col(\"label\").is_not_null())\n",
    "print(f\"Removed {initial_rows - df.height} rows with missing values.\")\n",
    "print(f\"Shape after cleaning: {df.shape}\")\n",
    "\n",
    "if df.height == 0:\n",
    "    raise ValueError(\"Error: No valid data remaining after filtering.\")\n",
    "\n",
    "# Verify labels\n",
    "unique_labels_in_data = set(df['label'].unique().to_list())\n",
    "print(f\"Unique labels found in data: {unique_labels_in_data}\")\n",
    "if not unique_labels_in_data.issubset(EXPECTED_LABELS):\n",
    "     raise ValueError(f\"Labels must only contain values within {EXPECTED_LABELS}. Found: {unique_labels_in_data}\")\n",
    "print(\"Labels are within the expected set.\")\n",
    "\n",
    "# Convert to Pandas (needed for some downstream steps like tokenizer list input and maybe class weights)\n",
    "pandas_df = df.to_pandas()\n",
    "print(\"Converted Polars DataFrame to Pandas DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Tokenization ---\n",
    "\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Tokenizing 'clean_content' (max_length={MAX_LENGTH})... This may take some time.\")\n",
    "\n",
    "# Tokenize the text data\n",
    "# Using .tolist() is often required as input to the tokenizer for batch processing\n",
    "tokenized_encodings = tokenizer(\n",
    "    pandas_df['clean_content'].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='tf' # Return TensorFlow tensors\n",
    ")\n",
    "\n",
    "# Extract tensors\n",
    "input_ids = tokenized_encodings['input_ids']\n",
    "attention_masks = tokenized_encodings['attention_mask']\n",
    "# Ensure labels are also tensors (and correct type)\n",
    "labels = tf.convert_to_tensor(pandas_df['label'].values, dtype=tf.int64)\n",
    "\n",
    "print(\"Tokenization complete.\")\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Attention Masks shape:\", attention_masks.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Free up memory from potentially large pandas_df if no longer needed\n",
    "# del pandas_df # Uncomment if memory is tight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Data Splitting ---\n",
    "\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "\n",
    "# Convert TensorFlow tensors to NumPy arrays *before* splitting\n",
    "# This is necessary because train_test_split uses NumPy indexing internally\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Split data using NumPy arrays\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    input_ids_np,\n",
    "    attention_masks_np,\n",
    "    labels_np,\n",
    "    test_size=TEST_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=labels_np  # Stratify to maintain label distribution in splits\n",
    ")\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "print(\"\\n--- Training Data Shapes ---\")\n",
    "print(f\"  Input IDs:      {train_inputs.shape}\")\n",
    "print(f\"  Attention Masks:{train_masks.shape}\")\n",
    "print(f\"  Labels:         {train_labels.shape}\")\n",
    "print(\"\\n--- Validation Data Shapes ---\")\n",
    "print(f\"  Input IDs:      {val_inputs.shape}\")\n",
    "print(f\"  Attention Masks:{val_masks.shape}\")\n",
    "print(f\"  Labels:         {val_labels.shape}\")\n",
    "\n",
    "# Optional: Check label distribution in splits\n",
    "# unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "# unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "# print(f\"\\nTrain label distribution: {dict(zip(unique_train, counts_train))}\")\n",
    "# print(f\"Validation label distribution: {dict(zip(unique_val, counts_val))}\")\n",
    "\n",
    "# Free up memory from the full numpy arrays if needed\n",
    "# del input_ids_np, attention_masks_np, labels_np # Uncomment if memory is tight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Calculate Class Weights ---\n",
    "\n",
    "# Calculate based on the *training* data labels to avoid data leakage from validation set\n",
    "print(\"Calculating class weights based on training data...\")\n",
    "unique_train_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "print(f\"Training label distribution: {dict(zip(unique_train_labels, counts))}\")\n",
    "\n",
    "# Use all expected labels for weight calculation, even if some are missing in this training split.\n",
    "# This prevents errors if a class has 0 samples in the training data.\n",
    "all_possible_classes = np.array(sorted(list(EXPECTED_LABELS)))\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=all_possible_classes, # Ensure all potential classes are considered\n",
    "    y=train_labels                # Calculate weights based on the actual training labels present\n",
    ")\n",
    "\n",
    "# Create dict mapping integer label to float weight\n",
    "class_weight_dict = dict(zip(all_possible_classes, class_weights))\n",
    "\n",
    "print(f\"Calculated class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3024f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 9: Model Training ---\n",
    "\n",
    "print(f\"Starting model training for {EPOCHS} epochs...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Using class weights: {class_weight_dict}\")\n",
    "\n",
    "# Prepare datasets in the format expected by model.fit\n",
    "# Input is a list or tuple containing the input tensors\n",
    "train_data_inputs = [train_inputs, train_masks]\n",
    "val_data_inputs = [val_inputs, val_masks]\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_inputs,       # Model inputs\n",
    "    train_labels,            # Target labels\n",
    "    validation_data=(val_data_inputs, val_labels), # Validation data tuple\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight_dict # Apply class weights to handle imbalance\n",
    ")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Optional: Plot training history\n",
    "# import matplotlib.pyplot as plt\n",
    "# pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0, 1) # set the y-axis range to [0,1]\n",
    "# plt.title(\"Model Training History\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 10: Model Evaluation ---\n",
    "\n",
    "print(\"Evaluating model on the validation set...\")\n",
    "\n",
    "# Prepare validation data input format\n",
    "val_data_inputs = [val_inputs, val_masks]\n",
    "\n",
    "eval_results = model.evaluate(\n",
    "    val_data_inputs,\n",
    "    val_labels,\n",
    "    batch_size=BATCH_SIZE, # Use a consistent batch size\n",
    "    return_dict=True # Returns results as a dictionary\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Example of accessing specific metrics:\n",
    "# print(f\"Validation Loss: {eval_results['loss']:.4f}\")\n",
    "# print(f\"Validation Accuracy: {eval_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0220ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 11: Save Model and Tokenizer ---\n",
    "\n",
    "print(f\"Saving model and tokenizer to directory: {OUTPUT_DIR}...\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save the trained model weights and configuration file\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save the tokenizer files\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Model and tokenizer saved successfully.\")\n",
    "print(f\"Files saved in: {os.path.abspath(OUTPUT_DIR)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
