{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check CUDA\n",
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Device Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def fine_tune_bert(train_texts, train_labels):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Class weights\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    class_weights = 1. / (class_counts + 1e-6)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\"cuda\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=4,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Tokenize text\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "\n",
    "    # Split indices for train/validation\n",
    "    indices = list(range(len(train_dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "    # Training arguments with early stopping\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,  # Reduced from 5\n",
    "        per_device_train_batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=100,  # Add warmup\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=1\n",
    "    )\n",
    "\n",
    "    class WeightedTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss = torch.nn.CrossEntropyLoss(weight=class_weights)(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        def on_epoch_end(self, args, state, control, **kwargs):\n",
    "            scheduler.step(self.state.log_history[-1]['eval_loss'])\n",
    "            super().on_epoch_end(args, state, control, **kwargs)\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=val_subset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_label(model, tokenizer, unlabeled_texts, confidence_threshold=0.95):  # Increased threshold\n",
    "    model.eval()\n",
    "    pseudo_texts = []\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    batch_size = 32\n",
    "    for i in range(0, len(unlabeled_texts), batch_size):\n",
    "        batch_texts = unlabeled_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=-1)\n",
    "            confidences, preds = torch.max(probs, dim=-1)\n",
    "        \n",
    "        mask = confidences > confidence_threshold\n",
    "        pseudo_texts.extend([batch_texts[j] for j in range(len(mask)) if mask[j]])\n",
    "        pseudo_labels.extend(preds[mask].cpu().numpy().astype(int).tolist())\n",
    "    \n",
    "    return pseudo_texts, pseudo_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution after imputation:\n",
      "label\n",
      "0    36\n",
      "1    35\n",
      "2    33\n",
      "3    17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Iteration 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.454200</td>\n",
       "      <td>1.362890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.353300</td>\n",
       "      <td>1.440123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.191700</td>\n",
       "      <td>1.513061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.169600</td>\n",
       "      <td>1.453206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.980300</td>\n",
       "      <td>1.436960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>1.626087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329400</td>\n",
       "      <td>1.363210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.192000</td>\n",
       "      <td>1.468374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>1.333840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>1.354270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>1.626087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329400</td>\n",
       "      <td>1.363210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.192000</td>\n",
       "      <td>1.468374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>1.333840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>1.354270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>1.626087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329400</td>\n",
       "      <td>1.363210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.192000</td>\n",
       "      <td>1.468374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>1.333840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>1.354270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>1.626087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329400</td>\n",
       "      <td>1.363210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.192000</td>\n",
       "      <td>1.468374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>1.333840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>1.354270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results Saved ===\n",
      "1. Final dataset: 'final_labeled_dataset.csv'\n",
      "2. Model: './final_model' directory\n",
      "3. Pseudo-labels: 'pseudo_labels_iteration_*.csv'\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Load labeled data\n",
    "    labeled_df = pd.read_csv(\"labeled_data.csv\")\n",
    "    \n",
    "    # Text preprocessing for labeled data\n",
    "    labeled_df[\"text\"] = labeled_df[\"JUDUL\"] + \" \" + labeled_df[\"OUTLET BERITA\"]\n",
    "    labeled_df[\"source\"] = \"original\"\n",
    "    labeled_df[\"original_index\"] = -1  # Mark as original data\n",
    "    \n",
    "    # Load unlabeled data\n",
    "    unlabeled_df = pd.read_csv(\"unlabeled_data.csv\")\n",
    "    unlabeled_df[\"text\"] = unlabeled_df[\"title\"] + \" \" + unlabeled_df[\"content\"]\n",
    "    unlabeled_df[\"source\"] = \"unlabeled\"\n",
    "    unlabeled_df[\"original_index\"] = unlabeled_df.index  # Track original indices\n",
    "    \n",
    "    # Track all pseudo-labeled indices\n",
    "    pseudo_indices = []\n",
    "    pseudo_labels = []\n",
    "\n",
    "    # Iterative bootstrapping\n",
    "    max_iterations = 3\n",
    "    iteration = 0\n",
    "    batch_size = 80\n",
    "    unlabeled_texts = unlabeled_df[\"text\"].tolist()\n",
    "    \n",
    "    while len(unlabeled_texts) > 0 and iteration < max_iterations:\n",
    "        print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
    "        \n",
    "        # Train model\n",
    "        model = fine_tune_bert(current_texts, current_labels)\n",
    "        \n",
    "        # Process batch\n",
    "        batch_texts = unlabeled_texts[:batch_size]\n",
    "        original_indices = unlabeled_df.iloc[:batch_size].index.tolist()  # Get original indices\n",
    "        \n",
    "        # Pseudo-labeling\n",
    "        new_texts, new_labels = pseudo_label(model, tokenizer, batch_texts)\n",
    "        \n",
    "        # Track pseudo-labeled indices and labels\n",
    "        pseudo_indices.extend(original_indices[:len(new_texts)])\n",
    "        pseudo_labels.extend(new_labels)\n",
    "        \n",
    "        # Remove processed texts (preserve original indices)\n",
    "        unlabeled_texts = unlabeled_texts[batch_size:]\n",
    "        unlabeled_df = unlabeled_df.iloc[batch_size:]  # Maintain original index tracking\n",
    "        \n",
    "        iteration += 1\n",
    "\n",
    "    # Build final ordered dataset\n",
    "    final_ordered = pd.concat([\n",
    "        labeled_df[[\"original_index\", \"text\", \"label\", \"source\"]],\n",
    "        unlabeled_df.loc[pseudo_indices].assign(\n",
    "            label=pseudo_labels,\n",
    "            source=\"pseudo\"\n",
    "        )\n",
    "    ]).sort_values(\"original_index\", ascending=True)\n",
    "\n",
    "    # Save results\n",
    "    final_ordered.to_csv(\"ordered_final_dataset.csv\", index=False)\n",
    "    print(\"\\n=== Results Saved ===\")\n",
    "    print(\"Final ordered dataset: 'ordered_final_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution after imputation:\n",
      "label\n",
      "0    36\n",
      "1    35\n",
      "2    33\n",
      "3    17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Iteration 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/60 00:08 < 01:14, 0.69 it/s, Epoch 0.58/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m model = \u001b[43mfine_tune_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Process batch with index tracking\u001b[39;00m\n\u001b[32m     51\u001b[39m batch_records = unlabeled_records[:batch_size]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mfine_tune_bert\u001b[39m\u001b[34m(train_texts, train_labels)\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[32m     57\u001b[39m trainer = WeightedTrainer(\n\u001b[32m     58\u001b[39m     model=model,\n\u001b[32m     59\u001b[39m     args=training_args,\n\u001b[32m     60\u001b[39m     train_dataset=train_subset,\n\u001b[32m     61\u001b[39m     eval_dataset=val_subset,\n\u001b[32m     62\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\transformers\\trainer.py:3740\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3738\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3740\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\accelerate\\accelerator.py:2359\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\semester_6\\NLP\\news_bias\\virtual\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Load labeled data\n",
    "    labeled_df = pd.read_csv(\"labeled_data.csv\")\n",
    "    \n",
    "    # Text preprocessing\n",
    "    labeled_df[\"JUDUL\"] = labeled_df[\"JUDUL\"].fillna(\"\").astype(str)\n",
    "    labeled_df[\"OUTLET BERITA\"] = labeled_df[\"OUTLET BERITA\"].fillna(\"\").astype(str)\n",
    "    labeled_df[\"text\"] = labeled_df[\"JUDUL\"] + \" \" + labeled_df[\"OUTLET BERITA\"]\n",
    "    \n",
    "    # Label preprocessing\n",
    "    labeled_df[\"label\"] = pd.to_numeric(labeled_df[\"ISI BERITA\"], errors=\"coerce\")\n",
    "    labeled_df[\"label\"] = labeled_df[\"label\"].fillna(2)  # Impute missing with 2\n",
    "    labeled_df[\"label\"] = labeled_df[\"label\"].astype(int)\n",
    "    \n",
    "    # Add metadata for original labeled data\n",
    "    labeled_df[\"source\"] = \"original\"\n",
    "    labeled_df[\"original_index\"] = -1  # Mark as original data\n",
    "    \n",
    "    # Verify labels\n",
    "    print(\"Label distribution after imputation:\")\n",
    "    print(labeled_df[\"label\"].value_counts())\n",
    "    \n",
    "    current_texts = labeled_df[\"text\"].tolist()\n",
    "    current_labels = labeled_df[\"label\"].tolist()\n",
    "    \n",
    "    # Load unlabeled data\n",
    "    unlabeled_df = pd.read_csv(\"unlabeled_data.csv\")\n",
    "    unlabeled_df[\"title\"] = unlabeled_df[\"title\"].fillna(\"\").astype(str)\n",
    "    unlabeled_df[\"content\"] = unlabeled_df[\"content\"].fillna(\"\").astype(str)\n",
    "    unlabeled_df[\"text\"] = unlabeled_df[\"title\"] + \" \" + unlabeled_df[\"content\"]\n",
    "    \n",
    "    # Track original indices\n",
    "    unlabeled_df[\"original_index\"] = unlabeled_df.index\n",
    "    unlabeled_records = unlabeled_df[[\"text\", \"original_index\"]].to_records(index=False).tolist()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    max_iterations = 3\n",
    "    iteration = 0\n",
    "    batch_size = 80\n",
    "    confidence_threshold = 0.95  # Start with high confidence\n",
    "    pseudo_history = []  # Track pseudo-labeled data\n",
    "\n",
    "    while len(unlabeled_records) > 0 and iteration < max_iterations:\n",
    "        print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
    "        \n",
    "        # Train model\n",
    "        model = fine_tune_bert(current_texts, current_labels)\n",
    "        \n",
    "        # Process batch with index tracking\n",
    "        batch_records = unlabeled_records[:batch_size]\n",
    "        batch_texts = [r[0] for r in batch_records]\n",
    "        batch_indices = [r[1] for r in batch_records]\n",
    "        \n",
    "        # Pseudo-labeling\n",
    "        new_texts, new_labels = pseudo_label(model, tokenizer, batch_texts, confidence_threshold)\n",
    "        \n",
    "        # Save pseudo-labels for this batch\n",
    "        pd.DataFrame({\n",
    "            \"original_index\": batch_indices,\n",
    "            \"text\": new_texts,\n",
    "            \"label\": new_labels,\n",
    "            \"iteration\": iteration + 1\n",
    "        }).to_csv(f\"pseudo_labels_iteration_{iteration + 1}.csv\", index=False)\n",
    "        \n",
    "        # Update datasets\n",
    "        current_texts.extend(new_texts)\n",
    "        current_labels.extend(new_labels)\n",
    "        pseudo_history.extend(zip(batch_indices, new_labels))\n",
    "        unlabeled_records = unlabeled_records[batch_size:]\n",
    "        \n",
    "        # Adjust confidence threshold dynamically\n",
    "        success_rate = len(new_texts) / batch_size\n",
    "        if success_rate < 0.3:  # If too few samples pass\n",
    "            confidence_threshold = max(0.85, confidence_threshold - 0.05)\n",
    "        \n",
    "        iteration += 1\n",
    "\n",
    "    # Create final ordered dataset\n",
    "    final_df = pd.concat([\n",
    "        labeled_df[[\"original_index\", \"text\", \"label\", \"source\"]],\n",
    "        pd.DataFrame({\n",
    "            \"original_index\": [idx for idx, _ in pseudo_history],\n",
    "            \"text\": [text for text, idx in unlabeled_records if idx in pseudo_history],\n",
    "            \"label\": [label for _, label in pseudo_history],\n",
    "            \"source\": \"pseudo\"\n",
    "        })\n",
    "    ])\n",
    "\n",
    "    # Sort by original index (original labels first, then pseudo in processing order)\n",
    "    final_df = final_df.sort_values(\"original_index\", ascending=True)\n",
    "    final_df.to_csv(\"ordered_final_dataset.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n=== Results Saved ===\")\n",
    "    print(\"1. Final dataset: 'ordered_final_dataset.csv'\")\n",
    "    print(\"2. Model: './final_model' directory\")\n",
    "    print(\"3. Pseudo-labels: 'pseudo_labels_iteration_*.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
